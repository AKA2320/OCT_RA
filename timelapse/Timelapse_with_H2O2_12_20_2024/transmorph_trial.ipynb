{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a906184d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from funcs_transmorph import *\n",
    "\n",
    "import pydicom as dicom\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import skimage as ski\n",
    "from skimage.transform import warp, AffineTransform, pyramid_expand, pyramid_reduce\n",
    "import cv2\n",
    "import scipy\n",
    "from natsort import natsorted\n",
    "\n",
    "from skimage.registration import phase_cross_correlation\n",
    "from scipy import ndimage as scp\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from scipy.optimize import minimize as minz\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "75053bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BrightestCenterSquareCrop(image):\n",
    "    H, W = image.shape\n",
    "    crop_size = min(H, W)\n",
    "\n",
    "    # Find brightest point\n",
    "    flat_idx = np.argmax(image)\n",
    "    y, x = divmod(flat_idx, W)\n",
    "\n",
    "    half = crop_size // 2\n",
    "    top = max(0, y - half)\n",
    "    left = max(0, x - half)\n",
    "\n",
    "    # Ensure bounds\n",
    "    if top + crop_size > H:\n",
    "        top = H - crop_size\n",
    "    if left + crop_size > W:\n",
    "        left = W - crop_size\n",
    "\n",
    "    cropped = image[top:top+crop_size, left:left+crop_size]\n",
    "    return cropped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e69705ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Define transform (same as training) ===\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # BrightestCenterSquareCrop(),\n",
    "    # transforms.Resize((64, 64)),\n",
    "])\n",
    "\n",
    "# === Load model on CPU ===\n",
    "model_path = '/Users/akapatil/Documents/Transmorph_2D_translation/model_transmorph_batch32_ncc_nonnormalized_shiftrange5.pt' # adjust path as needed\n",
    "model = torch.load(model_path, map_location='cpu')\n",
    "model.eval()\n",
    "\n",
    "# === Load Spatial Transformer (same size as training output) ===\n",
    "warper = SpatialTransformer(size=(64, 64))  # Match your training size\n",
    "warper.to('cpu')\n",
    "\n",
    "# === Inference function ===\n",
    "def infer(static_np, moving_np):\n",
    "    # Ensure float32 numpy arrays\n",
    "    static_np = static_np.astype(np.float32)\n",
    "    moving_np = moving_np.astype(np.float32)\n",
    "\n",
    "    # Preprocess\n",
    "    static = preprocess(static_np)\n",
    "    moving = preprocess(moving_np)\n",
    "\n",
    "    # Add batch and channel dim: (1, 1, H, W)\n",
    "    static = static.unsqueeze(0)\n",
    "    moving = moving.unsqueeze(0)\n",
    "\n",
    "    # Concat and infer\n",
    "    with torch.no_grad():\n",
    "        input_pair = torch.cat([static, moving], dim=1).double()  # shape: (1, 2, H, W)\n",
    "        moved_img, pred_translation = model(input_pair)\n",
    "        warped = warper(moving.double(), pred_translation)\n",
    "\n",
    "    # Remove batch + channel dim\n",
    "    warped_np = warped.squeeze().numpy()\n",
    "    return warped_np, pred_translation.squeeze().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9aa1b5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_dcm(scan_num,crop=False):\n",
    "    path = f'{scan_num}/'\n",
    "    pic_paths = []\n",
    "    for i in os.listdir(path):\n",
    "        if i.endswith('.dcm') or  i.endswith('.DCM') or i.endswith('.PNG'):\n",
    "            pic_paths.append(i)\n",
    "    pic_paths = np.array(natsorted(pic_paths))\n",
    "    fst = dicom.dcmread(path+pic_paths[0]).pixel_array\n",
    "\n",
    "    data = np.empty((len(pic_paths),fst.shape[0],fst.shape[1]))\n",
    "    for i,j in enumerate(pic_paths):\n",
    "        data[i] = dicom.dcmread(path+j).pixel_array\n",
    "    data = data[:,100:-100,:].astype(np.float32)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6459c4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data_dcm('scan20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f0c2bc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "static = np.zeros((64,64))\n",
    "moving = np.zeros((64,64))\n",
    "static[20:30,30:40] = 4.870\n",
    "moving[21:31,32:42] = 4.909"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "bb089bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " array([2.89223887, 0.84435774]))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer(static,moving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b08e70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ImageProc_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
